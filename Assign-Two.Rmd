---
title: "Assignment-Two"
output: html_document
date: "2022-10-20"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## U7117486 Assignment Two 

### Load in packages
```{r}
library(pacman)
p_load(bookdown, tidyverse, ggforce, flextable, latex2exp, png, magick, metafor) 
```

### Reading in the csv file 
```{r}
data <- read_csv("OA_activitydat_20190302_BIOL3207.csv")
data
```

### Omitting N/A data
```{r}
data <- na.omit(data)
data
```

### Drop irrelevant columns
```{r}
data <- subset(data, select = -c(comment, loc))
data
```

### Creating a summary of results 
```{r summaryTab}
summary1<- filter(data, treatment == "control") %>% group_by(species) %>% summarise(mean(activity), sd(activity), n())
colnames(summary1) <- c("Species", "ctrl.mean", "ctrl.sd", "ctrl.n")

summary2<- filter(data, treatment == "CO2") %>% group_by(species) %>% summarise(mean(activity), sd(activity), n())
colnames(summary2) <- c("Species", "oa.mean", "oa.sd", "oa.n")

summary <- merge(summary1, summary2)
summary
```
### Reading in the clark paper meta data
```{r}
clark_meta <- read_csv("clark_paper_data.csv")
clark_meta
```

### Binding the summary results to the study information meta data provided
```{r}
clark_data <- bind_cols(clark_meta, summary)
clark_data
```

### Reading in the larger meta data set 
```{r}
ocean_meta <- read_csv("ocean_meta_data.csv")
ocean_meta
```

### Binding the clark data to the larger meta data set 
```{r}
clark_data[, c(12)] <- sapply(clark_data[, c(12)], as.character)
clark_data[, c(7)] <- sapply(clark_data[, c(7)], as.character)
clark_data[, c(8)] <- sapply(clark_data[, c(8)], as.character)

merged_data <- bind_rows(ocean_meta, clark_data)

colnames(merged_data)[16] <- "Life_Stage"

# Getting rid of negative data
merged_data <- merged_data[which(merged_data$ctrl.n > 0),]
merged_data <- merged_data[which(merged_data$ctrl.mean > 0),]
merged_data <- merged_data[which(merged_data$ctrl.sd > 0),]
merged_data <- merged_data[which(merged_data$oa.n > 0),]
merged_data <- merged_data[which(merged_data$oa.mean > 0),]
merged_data <- merged_data[which(merged_data$oa.sd > 0),]

# Checking that negatives are excluded
max(merged_data$ctrl.mean,na.rm=FALSE)
min(merged_data$ctrl.mean,na.rm=FALSE)
max(merged_data$oa.mean,na.rm=FALSE)
min(merged_data$oa.mean,na.rm=FALSE)
max(merged_data$ctrl.n,na.rm=FALSE)
min(merged_data$ctrl.n,na.rm=FALSE)
max(merged_data$oa.n,na.rm=FALSE)
min(merged_data$oa.n,na.rm=FALSE)
```

### Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function.
```{r}
# Log Response Ratio (lnRR)
merged_data <- metafor::escalc(measure = "ROM", 
                               m1i = ctrl.mean,
                               sd1i = ctrl.sd, 
                               n1i = ctrl.n, 
                               m2i = oa.mean, 
                               sd2i = oa.sd, 
                               n2i = oa.n, 
                               data = merged_data, 
                               var.names = c("lnRR", "vlnRR"))

merged_data$residual <- 1:nrow(merged_data)
```
### Assessing data in tibble
```{r}
tibble(merged_data)
```
### Meta-analytic model fitted to data that controls for the sampling variance of lnRR - includes effect of study and observation. 
```{r, echo=TRUE}
## Meta Analytic Model fitted to the data - controls for sampling variance of lnRR.
MLMA <- rma.mv(lnRR ~ 1, 
               V = vlnRR,
               random = list(~1 | Study,
                             ~1 | residual), 
               dfs = "contain", 
               test = "t", 
               data = merged_data)
summary(MLMA)

r2 <- orchaRd::r2_ml(MLMA)
r2
```

### Predication Intervals
```{r}
prediction <- predict(MLMA)

pred <- as.data.frame(prediction)

```

### Meta Analytic Model Showing Effect Type
```{r V, echo=TRUE}
MAMET <- rma.mv(lnRR ~ 1 + Effect.type , 
                        V = vlnRR,
                        random = list(~1 | Study,
                                      ~1 | residual), 
                        dfs = "contain", 
                        test = "t", 
                        data = merged_data)
summary(MAMET)
```
### Proportion of total variability when excluding sampling variance 
```{r}
i2 <- tibble(orchaRd::i2_ml(MAMET, data = merged_data))
i2
```

### I2 results
```{r}
rma(yi = lnRR, 
    vi = vlnRR, 
    method = "DL", 
    data = merged_data)
```

### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure

```{r}
orchaRd::orchard_plot(MAMET, 
                      data = merged_data, 
                      group = "Study", 
                      mod = "Effect.type",
                      xlab = "Effect Type (lnRR)", 
                      angle = 45)
```
Figure 1: Forest Plot of Effect Types of Each Study in the "Merged Data" Dataset 

On average, there is an approximate `r x = pred[1,1]; x`% decrease in activity of fish for every 1% increase in ocean acidification when compared against the control. We are 95% confident that the true mean falls between `r x = pred[1,3]; x`% and `r x = pred[1,4]; x`%. There is a significant amount of heterogeneity among the effects (Q = 736,088,769, df = 763, p < 0.0001) with effect sizes expected to be as low as `r x = pred[1,5]; x`% and as high as `r x = pred[1,6]; x`%, 95% of the time (i2 total = `r x = i2[1,1]; x`%). The conditional R2 tells us that the full model explains `r x = i2[2,1]; x`% of variance in effect size, accounting for both fixed and random effects. The forest plot demonstrates that majority (50) of the studies published results with a strong effect type, with 427 samples across those studies. Studies with no effect type had the second most studies published (30) with 234 samples. And lastly, 11 of the studies published in this meta-analysis had a weak effect type, and only 103 samples.

### Funnel plot for visually assessing the possibility of publication bias.
```{r}
stderror <- sqrt(merged_data$vlnRR)

metafor::funnel(x = merged_data$lnRR,
                vi = stderror,
                yaxis = "seinv", 
                digits = 2, 
                xlim = c(-1, 1),
                levels = c(0.1, 0.05, 0.01),
                shade = c("white", "gray55", "gray 75"), 
                atransf=tanh, 
                legend = TRUE)

```
Figure 2: Funnel Plot of The Transformed Log Ratio of Means against the Inverse Standard Error

### Time-lag plot assessing how effect sizes may or may not have changed through time. -- Using Year of Print
```{r}
ggplot(merged_data, aes(y = lnRR, 
                        x = Year..print., 
                        size = 1/sqrt(vlnRR))) + 
                        geom_point(alpha = 0.3) +
                        geom_smooth(method = lm, 
                                    col = "red", 
                                    show.legend = FALSE) + 
                        labs(x = "Publication Year",
                        y = "Log Response Ratio (lnRR)", 
                        size = "Precision (1/SE)") +
                        theme_classic()
```
Figure 3: Time-Lag Plotof lnRR as a function of the Publication Year.

### Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
metareg_time <- rma.mv(lnRR ~ Year..print.,
                       V = vlnRR, 
                       random = list(~1 | Study,
                                     ~1 | residual),
                       test = "t", 
                       dfs = "contain", 
                       data = merged_data)
summary(metareg_time)
```

```{r}
r2_time <- orchaRd::r2_ml(metareg_time) 
r2_time
```

### Formal meta-regression model that includes inverse sampling variance (i.e., 1vlnRR) to test for file-drawer biases
```{r}
merged_data <- merged_data %>%
    mutate(Year_c = Year..print. - mean(Year..print.))

metareg_time_file <- rma.mv(lnRR ~ Year_c + vlnRR, 
                            V = vlnRR, 
                            random = list(~1 | Study,
                                          ~1 | residual),
                            test = "t", 
                            dfs = "contain", 
                            data = merged_data)
summary(metareg_time_file)
```


A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?
```{r}

```

time-lag: 
There does appear to be a clear positive relationship with year.
Also of note are that the earlier year studies have much higher sampling variance (i.e., lower precision), just like we might expect.
These early studies appear to have a lower (exaggerated) effect size compared with studies that are done in later years. 


funnel : 
We can see from Fig. 1 above a non-typical funnel shape. 
- most studies are flat against the bottom and not in the funnel - there are four studies that are outside of the funel - three to the left and one to the right 



Quantifying time lag bias using multilevel meta-regression: 
r2 marginal <- Time-lag explains `r x = r2_time[1,1]; x`% of the variation in lnRR. 
It’s clear that we have evidence of a time-lag bias???? (year increases, intercept decreases alot) The mean effect size is predicted to increase as more studies accumulate! But, wait, we also have evidence for other possible publication biases, such as ‘file-drawer’ effects. 
Great! That was easy. Just like any old linear mixed effect model we can just add moderators / fixed effects to the formula. Here, we can see that there is clear evidence, even when accounting for the covariance between the two, for both file-drawer and time-lag biases in these data.


With year 0 gone:
The overall mean correlation (r) when small sample and time-lag biases are controlled for is (intercept)

MLMA:
Yes, there is evidence for publication bias because the slope estimate for vlnRR is significant. We can see from this model that the adjusted lnRR when there is no uncertainty (i.e., the intercept) is -.1424 with a 95% confidence interval that overlaps zero (i.e., 95% CI = -0.3713 to 0.0865). 

