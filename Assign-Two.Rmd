---
title: "Assignment-Two"
output: html_document
date: "2022-10-20"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## U7117486 Assignment Two 


### Load in packages
```{r}
library(pacman)
p_load(bookdown, tidyverse, ggforce, flextable, latex2exp, png, magick, metafor) 
```


### Reading in the csv file 
```{r}
data <- read_csv("OA_activitydat_20190302_BIOL3207.csv")
data
```


### Omitting N/A data
```{r}
data <- na.omit(data)
data
```


### Drop irrelevant columns
```{r}
data <- subset(data, select = -c(comment, loc))
data
```


### Creating a summary of results 
```{r summaryTab}
summary1<- filter(data, treatment == "control") %>% group_by(species) %>% summarise(mean(activity), sd(activity), n())
colnames(summary1) <- c("Species", "ctrl.mean", "ctrl.sd", "ctrl.n")

summary2<- filter(data, treatment == "CO2") %>% group_by(species) %>% summarise(mean(activity), sd(activity), n())
colnames(summary2) <- c("Species", "oa.mean", "oa.sd", "oa.n")

summary <- merge(summary1, summary2)
summary
```


### Reading in the clark paper meta data
```{r}
clark_meta <- read_csv("clark_paper_data.csv")
clark_meta
```


### Binding the summary results to the study information meta data provided
```{r}
clark_data <- bind_cols(clark_meta, summary)
clark_data
```


###Reading in the larger meta data set 
```{r}
ocean_meta <- read_csv("ocean_meta_data.csv")
ocean_meta
```


### Binding the clark data to the larger meta data set 
```{r}
clark_data[, c(12)] <- sapply(clark_data[, c(12)], as.character)
clark_data[, c(7)] <- sapply(clark_data[, c(7)], as.character)
clark_data[, c(8)] <- sapply(clark_data[, c(8)], as.character)

merged_data <- bind_rows(ocean_meta, clark_data)

colnames(merged_data)[16] <- "Life_Stage"

# Getting rid of negative data
merged_data <- merged_data[which(merged_data$ctrl.n > 0),]
merged_data <- merged_data[which(merged_data$ctrl.mean > 0),]
merged_data <- merged_data[which(merged_data$ctrl.sd > 0),]
merged_data <- merged_data[which(merged_data$oa.n > 0),]
merged_data <- merged_data[which(merged_data$oa.mean > 0),]
merged_data <- merged_data[which(merged_data$oa.sd > 0),]

# Checking that negatives are excluded
max(merged_data$ctrl.mean,na.rm=FALSE)
min(merged_data$ctrl.mean,na.rm=FALSE)
max(merged_data$oa.mean,na.rm=FALSE)
min(merged_data$oa.mean,na.rm=FALSE)
max(merged_data$ctrl.n,na.rm=FALSE)
min(merged_data$ctrl.n,na.rm=FALSE)
max(merged_data$oa.n,na.rm=FALSE)
min(merged_data$oa.n,na.rm=FALSE)
```


### Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metaforâ€™s escalc() function.
```{r}
# Log Response Ratio (lnRR)
merged_data <- metafor::escalc(measure = "ROM", 
                               m1i = ctrl.mean,
                               sd1i = ctrl.sd, 
                               n1i = ctrl.n, 
                               m2i = oa.mean, 
                               sd2i = oa.sd, 
                               n2i = oa.n, 
                               data = merged_data, 
                               var.names = c("lnRR", "vlnRR"))

merged_data$residual <- 1:nrow(merged_data)
```


### Assessing data in tibble 
```{r}
tibble(merged_data)
```
Table 1: Tibble of Merged Data (Ocean Data and Clark Data) with means, sd and sample size for both treatments, as well as lnRR, vlnRR and residual columns. 


### Meta-analytic model fitted to data that controls for the sampling variance of lnRR - includes effect of study and observation. 
```{r, echo=TRUE}
## Meta Analytic Model fitted to the data - controls for sampling variance of lnRR.
MLMA <- rma.mv(lnRR ~ 1, 
               V = vlnRR,
               random = list(~1 | Study,
                             ~1 | residual,
                             ~1 | Effect.type), 
               dfs = "contain", 
               test = "t", 
               data = merged_data)
summary(MLMA)

r2 <- orchaRd::r2_ml(MLMA)
r2
```


### Predication Intervals
```{r}
prediction <- predict(MLMA)

pred <- as.data.frame(prediction)
pred
```
Table 2: Prediction Intervals of the MLMA Results


### Meta Analytic Model Showing Effect Type
```{r}
MAMET <- rma.mv(lnRR ~ Effect.type, 
                        V = vlnRR,
                        random = list(~1 | Study,
                                      ~1 | residual), 
                        dfs = "contain", 
                        test = "t", 
                        data = merged_data)
summary(MAMET)
```


### Proportion of total variability when excluding sampling variance 
```{r}
i2 <- tibble(orchaRd::i2_ml(MAMET, data = merged_data))
i2
```
Table 3: i2 Values of the MAMET Results


### I2 results
```{r}
rma(yi = lnRR, 
    vi = vlnRR, 
    method = "DL", 
    data = merged_data)
```


### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure

```{r}
orchaRd::orchard_plot(MAMET, 
                      data = merged_data, 
                      group = "Study", 
                      mod = "Effect.type",
                      xlab = "Effect Type (lnRR)", 
                      angle = 45)
```

Figure 1: Forest Plot of Effect Types of Each Study in the "Merged Data" Dataset 

On average, there is an approximate `r x = pred[1,1]; x`% decrease in activity of fish for every 1% increase in ocean acidification when compared against the control, this is shown in Table 2. We are 95% confident that the true mean falls between `r x = pred[1,3]; x`% and `r x = pred[1,4]; x`%. There is a significant amount of heterogeneity among the effects (Q = 736,088,769, df = 763, p < 0.0001) with effect sizes expected to be as low as `r x = pred[1,5]; x`% and as high as `r x = pred[1,6]; x`%, 95% of the time (i2 total = `r x = i2[1,1]; x`%). Found in Table 3, the conditional R2 tells us that the full model explains `r x = i2[2,1]; x`% of variance in effect size, accounting for both fixed and random effects. In Figure 1, the forest plot demonstrates that majority (51) of the studies published results with a strong effect type, with 420 samples across those studies. Studies with no effect type had the second most studies published (30) with 233 samples. And lastly, 11 of the studies published in this meta-analysis had a weak effect type, and only 102 samples.


### Funnel plot for vi
```{r}
metafor::funnel(x = merged_data$lnRR,
                vi = merged_data$vlnRR,
                yaxis = "seinv", 
                digits = 2, 
                ylim = c(1, 100),
                level = c(0.1, 0.05, 0.01),
                shade = c("white", "gray55", "gray 75"), 
                atransf=tanh, 
                legend = TRUE,
                ylab = "Precision (1/SE)", 
                xlab = "lnRR")

```

Figure 2: Funnel Plot of The Transformed Log Ratio of Means against the Precision, with a ylim of 100 to exclude outliers.


### Time-lag plot assessing how effect sizes may or may not have changed through time. -- Using Year of Print
```{r}
ggplot(merged_data, aes(y = lnRR, 
                        x = Year..print., 
                        size = 1/sqrt(vlnRR))) + 
                        geom_point(alpha = 0.3) +
                        geom_smooth(method = lm, 
                                    col = "red", 
                                    show.legend = FALSE) + 
                        labs(x = "Publication Year",
                        y = "Log Response Ratio (lnRR)", 
                        size = "Precision (1/SE)") +
                        theme_classic()
```

Figure 3: Time-Lag Plot of lnRR as a function of the Publication Year.

### Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
metareg_time <- rma.mv(lnRR ~ Year..print.,
                       V = vlnRR, 
                       random = list(~1 | Study,
                                     ~1 | residual),
                       test = "t", 
                       dfs = "contain", 
                       data = merged_data)
sumtime <- summary(metareg_time)
sumtime
```

Table 4: Summary of the Multivariate Meta-Analysis Model for Time-Lag Bias


```{r}
r2_time <- orchaRd::r2_ml(metareg_time) 
r2_tib <- tibble(r2_time)
r2_tib
```

Table 5: Variation Explained by Time When Results were Published in lnRR 


### Formal meta-regression model that includes inverse sampling variance (i.e., 1vlnRR) to test for file-drawer biases - also getting rid of Year 0
```{r}
merged_data <- merged_data %>%
    mutate(Year_c = Year..print. - mean(Year..print.))

metareg_time_file <- rma.mv(lnRR ~ Year_c + vlnRR, 
                            V = vlnRR, 
                            random = list(~1 | Study,
                                          ~1 | residual),
                            test = "t", 
                            dfs = "contain", 
                            data = merged_data)
sumfile <- summary(metareg_time_file)
sumfile
```


Table 6: Summary of the Multivariate Meta-Analysis Model for Time-Lag Bias with Variance Accounted For

```{r}
r2_time2 <- orchaRd::r2_ml(metareg_time_file) 
r2_tib2 <- tibble(r2_time2)
```

Table 7: Meta-Regression Model with Inverse Sampling Variance Testing for File-Drawer Bias


```{r}
lowsamp <- filter(merged_data, Average.n <= 10)
lowsamp_tib <- tibble(lowsamp)
lowsamp_tib
```
Table 8: Studies that have a sample size lower than 10

```{r}
highlnRR <- filter(merged_data, lnRR >= 5)
highlnRR_tib <- tibble(highlnRR)

lowlnRR <- filter(merged_data, lnRR <= -5)
lowlnRR_tib <- tibble(lowlnRR)

bindedlnRR <- bind_rows(highlnRR, lowlnRR)
bindedlnRR_tib <- tibble(bindedlnRR)
bindedlnRR_tib
```
Table 9: Studies with lnRR Values Under -5 or above +5.

The asymmetry of the funnel plot in Figure 1 demonstrates publication bias is present in this meta-analysis. The majority of the points are leaning towards the left side of the estimate, with there being an outlier in the grey-zone in the left side around 25 on the y-axis  The majority of the data points between 1 and 25 are bunched together on the left side. 

The Time-Lag Plot also demonstrates that there is publication bias present in the sample, it shows that there is a positive relationship between the average effect size and the year of publication, as they increase together. Studies in 2011 had a higher sampling variance than other years, and a bigger spread of lnRR. The earlier studies (before 2015) have higher data points for lnRR. In Table 5, the time-lag explains `r x = r2_tib[1,1]; x`% of the variation in lnRR, and when variance is accounted for, the time-lag explains `r x = r2_tib2[1,1]; x`% of the variation (Table 7). This could be seen as evidence of a time-lag bias as the mean effect size is expected to increase as more studies accumulate. By looking at Figure 3, it can be seen that as the years increase, the line is getting closer to 0 and therefore showing less bias over time. 

Using the centered on  the mean Year (Year_c), it can be seen that the intercept has an estimate of -0.1303% (Table 6) demonstrating the average correlation between effect size and activity. When Year 0 and variance wasn't accounted for, the intercept estimate was -264.0341% (Table 4). This estimate suggests, when accounting for both the variance and the time, that there is evidence to believe that there is time-lag and file-drawer bias present.

Contributing to the publication may be file-drawer bias as it can be seen from Tables 6 and 7 that there is publication bias evident, however time-lag was ruled out as the least likely contender. P-hacking could also be evident as a result of small sample sizes being used in the studies. Figure 1 shows that the data in the Strong Effect Size group has a smaller amount of samples per study on average than those in the Weak group (8.3 vs 10.3), this demonstrates potential for skewed data as a smaller sampling size allows for more chance of a statistical error.

In Table 8, studies with an average sample size lower than 10 is displayed - 10 was chosen as a random limit as the majority of the samples in this study have low sample sizes, so this is the smallest of them. The study "Irreversible behavioural impairment of fish starts early: Embryonic exposure to ocean acidification" has an avergage sample size of 4 and still published weak effect size - this is a prime example of a sample size that is far too small to properly detect any effect type. The control group in this study frequently had 4 samples, whilst the experimental group had 2 samples. Upon looking at Figure 1, it is evident that this study produced outlier data as when looking at the weak group there is an lnRR value at `r x = lowsamp_tib[95, 23]; x`, `r x = lowsamp_tib[101, 23]; x` and `r x = lowsamp_tib[104, 23]; x`, a far distance from the intercept estimate of -0.06 for the Weak effect group. 

Another source for this publication bias could be studies with dodgy methods of collecting/choosing only good data to publish. In Table 9, data that had an lnRR value above +5 or below -5 was selected, and studies by Dixson and Munday were notably frequent to create very strong effect sizes. The control groups mean and standard deviation are monumentally lower than that of in the experimental groups (e.g. Control Mean: `r x = bindedlnRR_tib[11, 18]; x` vs. Experimental Mean: `r x = bindedlnRR_tib[11, 21]; x`% time in cue, 10 dph, 700ppm). The results of the effects of Co2 on fish seem to be far larger than effect sizes in similar studies, which suggests that the fish in this study are extremely impacted by presence of Co2 in their environment. The Clark study "Ocean Acidification does not impair the behaviour of coral reef fishes" directly opposes the results found in Munday and Dixson's papers. The Clark study utilised a longer period of viewing the effects on the fish, and a much larger sample size (45 vs. 10-25). 

In the "Ocean acidification disrupts the innate ability of fish to detect predator olfactory cues" paper, there is possible repeated data, as the values for the "% time in predator cue, settlement stg, untreat vs.  pred 1" and "% time in predator cue, settlement stg, untreat vs.  pred 2" were identical, as well as the "% time in predator cue, settlement stg, pred 1 vs.  non-pred 1" and "% time in predator cue, settlement stg, non-pred 2 vs. pred 2". Unsurprisingly, these results yielded a "strong" effect type on Figure 1 and as seen in Table 9. 

In the "Replenishment of fish populations is threatened by ocean acidification" paper, repeated zeroed data in the ctrl.mean column has lead to high lnRR values, creating strong effect sizes as there is always going to be a large difference from 0 (study data has made the size `r x = bindedlnRR_tib[11, 17]; x` to get rid of zeroed data) to any other number measured. This repeated data goes across 18 rows, 11 of which were in Table 9 (the other 7 had a 0 for the oa.mean and therefore created no effect as there was 0 difference in the results, and the lnRR was 0). All 11 that created an effect size were strong effect sizes, and are all seen in Figure 1 as outliers. 

