---
title: "Assignment-Two"
output: html_document
date: "2022-10-20"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## U7117486 Assignment Two 


### Load in packages
```{r}
library(pacman)
p_load(bookdown, tidyverse, ggforce, flextable, latex2exp, png, magick, metafor) 
```


### Reading in the csv file 
```{r}
data <- read_csv("OA_activitydat_20190302_BIOL3207.csv")
data
```


### Omitting N/A data
```{r}
data <- na.omit(data)
data
```


### Drop irrelevant columns
```{r}
data <- subset(data, select = -c(comment, loc))
data
```


### Creating a summary of results 
```{r summaryTab}
summary1<- filter(data, treatment == "control") %>% group_by(species) %>% summarise(mean(activity), sd(activity), n())
colnames(summary1) <- c("Species", "ctrl.mean", "ctrl.sd", "ctrl.n")

summary2<- filter(data, treatment == "CO2") %>% group_by(species) %>% summarise(mean(activity), sd(activity), n())
colnames(summary2) <- c("Species", "oa.mean", "oa.sd", "oa.n")

summary <- merge(summary1, summary2)
summary
```


### Reading in the clark paper meta data
```{r}
clark_meta <- read_csv("clark_paper_data.csv")
clark_meta
```


### Binding the summary results to the study information meta data provided
```{r}
clark_data <- bind_cols(clark_meta, summary)
clark_data
```


###Reading in the larger meta data set 
```{r}
ocean_meta <- read_csv("ocean_meta_data.csv")
ocean_meta
```


### Binding the clark data to the larger meta data set 
```{r}
clark_data[, c(12)] <- sapply(clark_data[, c(12)], as.character)
clark_data[, c(7)] <- sapply(clark_data[, c(7)], as.character)
clark_data[, c(8)] <- sapply(clark_data[, c(8)], as.character)

merged_data <- bind_rows(ocean_meta, clark_data)

colnames(merged_data)[16] <- "Life_Stage"

# Getting rid of negative data
merged_data <- merged_data[which(merged_data$ctrl.n > 0),]
merged_data <- merged_data[which(merged_data$ctrl.mean > 0),]
merged_data <- merged_data[which(merged_data$ctrl.sd > 0),]
merged_data <- merged_data[which(merged_data$oa.n > 0),]
merged_data <- merged_data[which(merged_data$oa.mean > 0),]
merged_data <- merged_data[which(merged_data$oa.sd > 0),]

# Checking that negatives are excluded
max(merged_data$ctrl.mean,na.rm=FALSE)
min(merged_data$ctrl.mean,na.rm=FALSE)
max(merged_data$oa.mean,na.rm=FALSE)
min(merged_data$oa.mean,na.rm=FALSE)
max(merged_data$ctrl.n,na.rm=FALSE)
min(merged_data$ctrl.n,na.rm=FALSE)
max(merged_data$oa.n,na.rm=FALSE)
min(merged_data$oa.n,na.rm=FALSE)
```


### Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metaforâ€™s escalc() function.
```{r}
# Log Response Ratio (lnRR)
merged_data <- metafor::escalc(measure = "ROM", 
                               m1i = ctrl.mean,
                               sd1i = ctrl.sd, 
                               n1i = ctrl.n, 
                               m2i = oa.mean, 
                               sd2i = oa.sd, 
                               n2i = oa.n, 
                               data = merged_data, 
                               var.names = c("lnRR", "vlnRR"))

merged_data$residual <- 1:nrow(merged_data)
```


### Assessing data in tibble 
```{r}
tibble(merged_data)

```
Table 1: Tibble of Merged Data (Ocean Data and Clark Data) with means, sd and sample size for both treatments, as well as lnRR, vlnRR and residual columns. 


### Meta-analytic model fitted to data that controls for the sampling variance of lnRR - includes effect of study and observation. 
```{r, echo=TRUE}
## Meta Analytic Model fitted to the data - controls for sampling variance of lnRR.
MLMA <- rma.mv(lnRR ~ 1, 
               V = vlnRR,
               random = list(~1 | Study,
                             ~1 | residual,
                             ~1 | Effect.type), 
               dfs = "contain", 
               test = "t", 
               data = merged_data)
summary(MLMA)

r2 <- orchaRd::r2_ml(MLMA)
r2
```


### Predication Intervals
```{r}
prediction <- predict(MLMA)

pred <- as.data.frame(prediction)
pred
```
Table 2: Prediction Intervals of the MLMA Results


### Meta Analytic Model Showing Effect Type
```{r}
MAMET <- rma.mv(lnRR ~ Effect.type, 
                        V = vlnRR,
                        random = list(~1 | Study,
                                      ~1 | residual), 
                        dfs = "contain", 
                        test = "t", 
                        data = merged_data)
summary(MAMET)
```


### Proportion of total variability when excluding sampling variance 
```{r}
i2 <- tibble(orchaRd::i2_ml(MAMET, data = merged_data))
i2
```
Table 3: i2 Values of the MAMET Results


### I2 results
```{r}
rma(yi = lnRR, 
    vi = vlnRR, 
    method = "DL", 
    data = merged_data)
```


### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure

```{r}
orchaRd::orchard_plot(MAMET, 
                      data = merged_data, 
                      group = "Study", 
                      mod = "Effect.type",
                      xlab = "Effect Type (lnRR)", 
                      angle = 45)
```

Figure 1: Forest Plot of Effect Types of Each Study in the "Merged Data" Dataset 

On average, there is an approximate `r x = pred[1,1]; x`% decrease in activity of fish for every 1% increase in ocean acidification when compared against the control, this is shown in Table 2. We are 95% confident that the true mean falls between `r x = pred[1,3]; x`% and `r x = pred[1,4]; x`%. There is a significant amount of heterogeneity among the effects (Q = 736,088,769, df = 763, p < 0.0001) with effect sizes expected to be as low as `r x = pred[1,5]; x`% and as high as `r x = pred[1,6]; x`%, 95% of the time (i2 total = `r x = i2[1,1]; x`%). Found in Table 3, the conditional R2 tells us that the full model explains `r x = i2[2,1]; x`% of variance in effect size, accounting for both fixed and random effects. In Figure 1, the forest plot demonstrates that majority (51) of the studies published results with a strong effect type, with 420 samples across those studies. Studies with no effect type had the second most studies published (30) with 233 samples. And lastly, 11 of the studies published in this meta-analysis had a weak effect type, and only 102 samples.


### Funnel plot for vi
```{r}
metafor::funnel(x = merged_data$lnRR,
                vi = merged_data$vlnRR,
                yaxis = "seinv", 
                digits = 2, 
                ylim = c(1, 100),
                level = c(0.1, 0.05, 0.01),
                shade = c("white", "gray55", "gray 75"), 
                atransf=tanh, 
                legend = TRUE,
                ylab = "Precision (1/SE)", 
                xlab = "lnRR")

```

Figure 2: Funnel Plot of The Transformed Log Ratio of Means against the Precision, with a ylim of 100 to exclude outliers.


### Time-lag plot assessing how effect sizes may or may not have changed through time. -- Using Year of Print
```{r}
ggplot(merged_data, aes(y = lnRR, 
                        x = Year..print., 
                        size = 1/sqrt(vlnRR))) + 
                        geom_point(alpha = 0.3) +
                        geom_smooth(method = lm, 
                                    col = "red", 
                                    show.legend = FALSE) + 
                        labs(x = "Publication Year",
                        y = "Log Response Ratio (lnRR)", 
                        size = "Precision (1/SE)") +
                        theme_classic()
```

Figure 3: Time-Lag Plot of lnRR as a function of the Publication Year.

### Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
metareg_time <- rma.mv(lnRR ~ Year..print.,
                       V = vlnRR, 
                       random = list(~1 | Study,
                                     ~1 | residual),
                       test = "t", 
                       dfs = "contain", 
                       data = merged_data)
sumtime <- summary(metareg_time)
sumtime
```

Table 4: Summary of the Multivariate Meta-Analysis Model for Time-Lag Bias


```{r}
r2_time <- orchaRd::r2_ml(metareg_time) 
r2_tib <- tibble(r2_time)
r2_tib
```

Table 5: Variation Explained by Time When Results were Published in lnRR 


### Formal meta-regression model that includes inverse sampling variance (i.e., 1vlnRR) to test for file-drawer biases - also getting rid of Year 0
```{r}
merged_data <- merged_data %>%
    mutate(Year_c = Year..print. - mean(Year..print.))

metareg_time_file <- rma.mv(lnRR ~ Year_c + vlnRR, 
                            V = vlnRR, 
                            random = list(~1 | Study,
                                          ~1 | residual),
                            test = "t", 
                            dfs = "contain", 
                            data = merged_data)
sumfile <- summary(metareg_time_file)
sumfile
```


Table 6: Summary of the Multivariate Meta-Analysis Model for Time-Lag Bias with Variance Accounted For

```{r}
r2_time2 <- orchaRd::r2_ml(metareg_time_file) 
r2_tib2 <- tibble(r2_time2)
```

Table 7: Meta-Regression Model with Inverse Sampling Variance Testing for File-Drawer Bias



A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?



The asymmetry of the funnel plot in Figure 1 demonstrates publication bias is present in this meta-analysis. The majority of the points are leaning towards the left side of the estimate, with there being an outlier in the grey-zone in the left side around 25 on the y-axis  The majority of the data points between 1 and 25 are bunched together on the left side. 

The Time-Lag Plot also demonstrates that there is publication bias present in the sample, it shows that there is a positive relationship between the average effect size and the year of publication, as they increase together. Studies in 2011 had a higher sampling variance than other years, and a bigger spread of lnRR. The earlier studies (before 2015) have higher data points for lnRR. In Table 5, the time-lag explains `r x = r2_tib[1,1]; x`% of the variation in lnRR, and when variance is accounted for, the time-lag explains `r x = r2_tib2[1,1]; x`% of the variation (Table 7). This could be seen as evidence of a time-lag bias as the mean effect size is expected to increase as more studies accumulate. 

Using the centred on  the mean Year (Year_c), it can be seen that the intercept has an estimate of -0.1303% (Table 6) demonstrating the average correlation between effect size and activity. When Year 0 and variance wasn't accounted for, the intercept estimate was -264.0341% (Table 4). This estimate suggests, when accounting for both the variance and the time, that there is evidence to believe that there is time-lag and file-drawer bias present.



Itâ€™s clear that we have evidence of a time-lag bias. The mean effect size is predicted to decrease as more studies accumulate! But, wait, we also have evidence for other possible publication biases, such as â€˜file-drawerâ€™ effects. We can actually create a model that accounts for both of these effects, and that accounts for the possible covariance between the two (i.e., sampling variance is expected to be high in early years). Letâ€™s fit that model.




